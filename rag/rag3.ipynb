{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c24b70-2d8b-4655-8a5a-1a46f2f95c0c",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db0bfb5-065f-4cf8-9d7e-8ff9f71e1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f470768-07dd-40c5-98d6-b239b6c437d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [item['context'] for item in dataset['train']]\n",
    "u_contexts = list(set(contexts))\n",
    "truncated_contexts = u_contexts[:12800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35acf48d-ec3c-4e0e-8567-36acf6c7d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def chunky_iterate(data, chunk_size):\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i: i + chunk_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2563726-a1e7-4b7a-93b1-d8d01d08f792",
   "metadata": {},
   "source": [
    "## embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d6437d-77b1-4601-8f85-eac5edf299c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    optimizations:\n",
    "        * use a cache folder to cache the model\n",
    "        * generate embeddings in a batch at a time.\n",
    "\"\"\"\n",
    "class Embedder:\n",
    "    def __init__(self, model_name = 'nomic-ai/nomic-embed-text-v1.5',\n",
    "                batch_size = 32):\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.embed_model = self._load_embed_model()\n",
    "\n",
    "    def _load_embed_model(self):\n",
    "        return HuggingFaceEmbedding(model_name= self.model_name,\n",
    "            trust_remote_code= True, cache_folder= './hf_cache')\n",
    "\n",
    "    \n",
    "    def generate_batch_embeddings(self, context_batch):\n",
    "        return self.embed_model.get_text_embedding_batch(context_batch)\n",
    "    \n",
    "\n",
    "    def generate_embeddings(self, contexts):\n",
    "        embeddings = []\n",
    "        for batch_context in chunky_iterate(contexts, self.batch_size):\n",
    "            embeddings.extend(self.generate_batch_embeddings(batch_context))\n",
    "        return embeddings\n",
    "\n",
    "    def generate_query_embeddings(self, query):\n",
    "        return self.embed_model.get_query_embedding(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02c2bf-a4b3-4a12-bfe0-757410460614",
   "metadata": {},
   "source": [
    "## vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a161c6-a4a5-4b1a-b46f-f7f5f8e91412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\"\"\"\n",
    "    Optimizations:\n",
    "        * use grpc\n",
    "        * collection on disk for larger dataset\n",
    "        * upload collection batches upload automatically.\n",
    "        * set indexing to false / 0 when uploading large data for the first time\n",
    "        \n",
    "\"\"\"\n",
    "class QdrantVectorDB:\n",
    "    def __init__(self, collection_name = \"contexts_collection\",\n",
    "                 vector_dim = 768, batch_size = 512,\n",
    "                 host = 'localhost', port = 6333):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.collection_name = collection_name\n",
    "        self.client = self._setup_client(host, port)\n",
    "        self._setup_collection(vector_dim)\n",
    "\n",
    "    def _setup_client(self, host, port):\n",
    "        return QdrantClient(host= host, port= port, prefer_grpc=True) \n",
    "\n",
    "    def _setup_collection(self, vector_dim):\n",
    "        if not self.client.collection_exists(collection_name= self.collection_name):\n",
    "            self.client.create_collection(\n",
    "                collection_name= self.collection_name,\n",
    "                vectors_config= models.VectorParams(size = vector_dim,\n",
    "                    distance = models.Distance.DOT, on_disk=True\n",
    "                ),\n",
    "                optimizers_config=models.OptimizersConfigDiff(\n",
    "                    default_segment_number = 5, indexing_threshold=0)\n",
    "            ),\n",
    "            \n",
    "    def ingest_data(self, embeddings, contexts):\n",
    "             \n",
    "        self.client.upload_collection(collection_name= self.collection_name,\n",
    "            vectors= embeddings, batch_size= self.batch_size,\n",
    "            payload=[{\"context\": context} for context in contexts]\n",
    "        )\n",
    "        \n",
    "        self.client.update_collection(collection_name=self.collection_name,\n",
    "            optimizers_config=models.OptimizersConfigDiff(indexing_threshold=20000))\n",
    "            \n",
    "    def query_collection(self, query_embeddings):\n",
    "        result = self.client.query_points(\n",
    "            collection_name = self.collection_name,\n",
    "            query= query_embeddings,\n",
    "            search_params = models.SearchParams(\n",
    "                    quantization = models.QuantizationSearchParams(\n",
    "                        ignore = False,\n",
    "                        rescore = True,\n",
    "                        oversampling = 2.0\n",
    "                    )\n",
    "            ),\n",
    "            timeout = 1000\n",
    "        )\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb9d15-6067-4473-b435-edd5352c88cd",
   "metadata": {},
   "source": [
    "## retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c57d53-a228-4ee7-8438-7f1d07334a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, vectordb, embedder):\n",
    "        self.vectordb = vectordb\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def search(self, query):\n",
    "        query_embeddings = self.embedder.generate_query_embeddings(query)\n",
    "        start_time = time.time()\n",
    "\n",
    "        result = self.vectordb.query_collection(query_embeddings)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Execution of the query took : {elapsed_time} seconds')\n",
    "        return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01471cbb-3fa6-4377-8754-a242e1d7fd32",
   "metadata": {},
   "source": [
    "## Rag class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7904f82-2281-4d27-8d05-5746bd931b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, retriever, llm_model = 'llama3.2:1b'):\n",
    "        self.retriever = retriever\n",
    "        self.llm = self._setup_llm(llm_model)\n",
    "        self.qa_prompt_template = self._setup_prompt_template()\n",
    "\n",
    "    def _setup_llm(self, llm_model):\n",
    "        return Ollama(model=llm_model)\n",
    "\n",
    "    def _setup_prompt_template(self):\n",
    "        return \"\"\"\n",
    "            Context information is below\n",
    "            ----------------------------\n",
    "            {context}\n",
    "            ----------------------------\n",
    "\n",
    "            Given the context information above i want you \n",
    "            to think step by step to answer the query in a \n",
    "            crisp manner, in case you don't know the answer,\n",
    "            say 'I don't know'\n",
    "\n",
    "            ------------------------------\n",
    "            Query: {query}\n",
    "            ------------------------------\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "    def generate_context(self, query):\n",
    "        result = self.retriever.search(query)\n",
    "        contexts = [item.payload['context'] for item in result.points]\n",
    "        return '\\n\\n---\\n\\n'.join(contexts)\n",
    "       \n",
    "\n",
    "    def query(self, query):\n",
    "        contexts = self.generate_context(query=query)\n",
    "        prompt = self.qa_prompt_template.format(context=contexts, query=query)\n",
    "        response = self.llm.complete(prompt)\n",
    "        return dict(response)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4469a9d-6b56-4342-9db2-cb0809f43b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "embedder = Embedder()\n",
    "embeddings = embedder.generate_embeddings(truncated_contexts)\n",
    "\n",
    "vectordb = QdrantVectorDB()\n",
    "vectordb.ingest_data(embeddings=embeddings, contexts=truncated_contexts)\n",
    "\n",
    "retriever = Retriever(vectordb=vectordb, embedder=embedder)\n",
    "\n",
    "rag = RAG(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6604ee3d-ffba-47a0-8275-f51a5b158512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution of the query took : 0.010528802871704102 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"The premium and VIP services in Airports are reserved for which type of passengers?\"\n",
    "result = rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4543b4-5c9d-4599-8c7c-45ea22d59403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine which type of passenger is eligible for premium and VIP services at airports, we need to consider the following steps:\n",
       "\n",
       "1. Identify the definition of premium and VIP services.\n",
       "2. Analyze the context where these services are mentioned (e.g., in the provided text).\n",
       "3. Determine which passengers or groups are typically considered as such.\n",
       "\n",
       "Based on the given information, we can infer that:\n",
       "\n",
       "- Premium and VIP services often require a high level of customer service or luxury experience. This suggests that they might be more suitable for business travelers, first-class passengers, or those who value exceptional comfort and convenience.\n",
       "- Business class passengers are typically willing to pay a premium for their travel experience, indicating that these individuals may be more likely to benefit from the services offered.\n",
       "\n",
       "Considering these points, it can be inferred that:\n",
       "\n",
       "- First-class passengers at airports would likely be eligible for premium and VIP services. They often pay a higher fare than business class passengers and may expect a more luxurious experience during their flight.\n",
       "- Business class passengers, who are typically willing to pay a premium for their travel experience, might also benefit from the available premium and VIP services.\n",
       "\n",
       "Therefore, based on this analysis, I would say that:\n",
       "\n",
       "I don't know"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecab27-70f4-4c43-9db1-a1478c6da82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
